import os
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback

# å¯¼å…¥æˆ‘ä»¬å®šä¹‰çš„ Env (å‡è®¾ä½ æŠŠä¸Šé¢çš„ Env ä»£ç å­˜ä¸ºäº† env_meta.py)
from env_meta import MetaSyntheticEnv 
from dmpo_model import MetaDMPOActionWrapper

def main():
    log_dir = "logs_meta_dmpo"
    os.makedirs(log_dir, exist_ok=True)
    
    # 1. åˆ›å»ºå¹¶åŒ…è£…ç¯å¢ƒ
    def make_env():
        env = MetaSyntheticEnv(n_assets=5, n_steps=2000, lookback=30)
        env = MetaDMPOActionWrapper(env) # åŠ ä¸Šæˆ‘ä»¬çš„ Meta é€»è¾‘
        env = Monitor(env, log_dir)
        return env
    
    # å‘é‡åŒ–ç¯å¢ƒ
    env = DummyVecEnv([make_env])
    
    # å½’ä¸€åŒ–æ˜¯å¿…é¡»çš„ï¼Œå› ä¸ºé‡‘èæ•°æ®çš„ Reward å°ºåº¦å¾ˆå°
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)
    
    # 2. å®šä¹‰æ¨¡å‹ (ä½¿ç”¨ç®€å•çš„ MLP)
    # policy="MlpPolicy" ä¼šè‡ªåŠ¨æ„å»ºå‡ ä¸ªå…¨è¿æ¥å±‚
    model = PPO(
        "MlpPolicy", 
        env,
        verbose=1,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        ent_coef=0.01, # ç¨å¾®åŠ ç‚¹ç†µæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ—©æ”¶æ•›åˆ°å•ä¸€ Alpha
        tensorboard_log=log_dir
    )
    
    # 3. è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ Meta-DMPO Agent...")
    model.learn(total_timesteps=100000) # è·‘ 100k æ­¥è¯•æ°´
    
    # 4. ä¿å­˜
    model.save("meta_dmpo_agent")
    env.save("meta_vec_normalize.pkl") # å¿…é¡»ä¿å­˜å½’ä¸€åŒ–å‚æ•°ï¼
    print("âœ… æ¨¡å‹å·²ä¿å­˜")

if __name__ == "__main__":
    main()