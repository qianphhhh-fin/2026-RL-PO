# train.py
import os
import numpy as np
# from stable_baselines3 import PPO,SAC
from sbx import SAC
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor
# å¼•å…¥ BaseCallback ç”¨äºè‡ªå®šä¹‰è®°å½•
from stable_baselines3.common.callbacks import BaseCallback 

from env_meta import MetaSyntheticEnv 
from dmpo_model import MetaDMPOActionWrapper

# --- æ–°å¢ï¼šè‡ªå®šä¹‰ Callback ç±» ---
class OracleWeightLogger(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
    
    def _on_step(self) -> bool:
        # self.locals['infos'] åŒ…å«äº†å½“å‰æ­¥æ‰€æœ‰ç¯å¢ƒçš„ info åˆ—è¡¨
        infos = self.locals['infos']
        
        # éå†æ‰€æœ‰ç¯å¢ƒï¼ˆé€šå¸¸ DummyVecEnv åªæœ‰ä¸€ä¸ªç¯å¢ƒï¼‰
        for info in infos:
            if 'alpha_weights' in info:
                weights = info['alpha_weights']
                # è®°å½• Oracle (Index 0) çš„æƒé‡
                # "custom/oracle_weight" ä¼šå‡ºç°åœ¨ TensorBoard çš„ custom æ ‡ç­¾ä¸‹
                self.logger.record("custom/oracle_weight", weights[0])
                
                # å¦‚æœä½ æƒ³çœ‹åæŒ‡ç­–ç•¥ (Index 1) çš„æƒé‡ä¹Ÿå¯ä»¥åŠ ä¸Šï¼š
                self.logger.record("custom/inverse_weight", weights[1])
                
                # å¦‚æœä½ æƒ³çœ‹ Noise (Index 2)
                self.logger.record("custom/noise_weight", weights[2])

                # å¦‚æœä½ æƒ³çœ‹ Regime (Index 3)
                self.logger.record("custom/regime_weight", weights[3])

        return True

def main():
    log_dir = "logs_meta_dmpo" # æ”¹ä¸ªåå­—åŒºåˆ†
    os.makedirs(log_dir, exist_ok=True)
    
    # 1. ç¯å¢ƒè®¾ç½® (ä¿æŒä¸å˜)
    def make_env():
        # å»ºè®®é…åˆä¹‹å‰çš„ä¿®æ”¹ï¼šé£é™©ä¸­æ€§ Solver + å¢åŠ  MSE ç‰¹å¾
        env = MetaSyntheticEnv(n_assets=5, n_steps=2000, lookback=30)
        env = MetaDMPOActionWrapper(env)
        env = Monitor(env, log_dir)
        return env
    
    env = DummyVecEnv([make_env])
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)
    
    # 2. å®šä¹‰ SAC æ¨¡å‹
    # SAC ä¸éœ€è¦ n_steps (å®ƒæ˜¯ Off-policy)ï¼Œè€Œæ˜¯ç”¨ buffer_size
    model = SAC(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=3e-4,
        buffer_size=100000, # ç»éªŒå›æ”¾æ± å¤§å°
        batch_size=256,     # SAC é€šå¸¸ç”¨å¤§ä¸€ç‚¹çš„ Batch
        ent_coef='auto',    # å…³é”®ï¼è®©å®ƒè‡ªåŠ¨è°ƒæ•´æ¢ç´¢åŠ›åº¦
        policy_kwargs=dict(net_arch=[64,64]), 
        train_freq=4,       # æ¯ä¸ª step éƒ½è®­ç»ƒ
        gradient_steps=4,   # æ¯æ¬¡æ›´æ–°ä¸€æ­¥
        tensorboard_log=log_dir,
        device='cpu'  # å¦‚æœæœ‰ GPU å¯ä»¥æ”¹æˆ 'cuda'
    )
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ Meta-DMPO (SAC ç‰ˆ)...")
    
    # å› ä¸º SAC è®­ç»ƒæ›´æ…¢ï¼ˆæ¯ä¸ª step éƒ½åå‘ä¼ æ’­ï¼‰ï¼ŒåŒæ ·çš„ total_timesteps ä¼šæ¯” PPO æ…¢
    # ä½†å®ƒçš„æ”¶æ•›é€šå¸¸éœ€è¦æ›´å°‘çš„ steps
    model.learn(total_timesteps=200000, callback=OracleWeightLogger(),log_interval=1) 
    
    model.save("meta_dmpo_sac")
    env.save("meta_vec_normalize_sac.pkl")

if __name__ == "__main__":
    main()