import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

from env_meta import MetaSyntheticEnv
from dmpo_model import MetaDMPOActionWrapper

def run_test():
    # 1. å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
    # æ³¨æ„ï¼šæµ‹è¯•æ—¶ä¸è¦ Shuffleï¼Œè¦æŒ‰é¡ºåº
    raw_env = MetaSyntheticEnv(n_assets=5, n_steps=1000, lookback=30)
    env = MetaDMPOActionWrapper(raw_env)
    env = DummyVecEnv([lambda: env])
    
    # åŠ è½½å½’ä¸€åŒ–å‚æ•° (éå¸¸é‡è¦ï¼Œå¦åˆ™ Agent çœ‹ä¸æ‡‚æ•°æ®)
    env = VecNormalize.load("meta_vec_normalize.pkl", env)
    env.training = False # æµ‹è¯•æ¨¡å¼ï¼Œä¸æ›´æ–°å‡å€¼æ–¹å·®
    env.norm_reward = False
    
    # åŠ è½½æ¨¡å‹
    model = PPO.load("meta_dmpo_agent")
    
    print("ğŸ“Š å¼€å§‹æµ‹è¯•è¿è¡Œ...")
    obs = env.reset()
    
    # è®°å½•æ•°æ®
    portfolio_values = [1.0]
    alpha_weights_history = []
    
    # æ‹¿åˆ°åº•å±‚ç¯å¢ƒå¼•ç”¨ç”¨äº Benchmark
    base_env = env.envs[0].env.unwrapped
    market_returns = base_env.returns # çœŸå®å¸‚åœºæ”¶ç›Š
    
    done = False
    step_idx = 0
    
    while not done:
        # é¢„æµ‹åŠ¨ä½œ
        action, _ = model.predict(obs, deterministic=True)
        obs, rewards, dones, infos = env.step(action)
        
        # è®°å½•å‡€å€¼
        ret = infos[0].get('terminal_observation', {}) # SB3 çš„ quirkï¼Œå¿½ç•¥
        # æˆ‘ä»¬æ‰‹åŠ¨ç®—ç´¯è®¡æ”¶ç›Šæ›´å‡†
        # è¿™é‡Œç”¨ä¸€ç§ç®€å•è¿‘ä¼¼ï¼šåˆ©ç”¨è®°å½•çš„ info
        
        # å®é™…ä¸Š VecEnv çš„ step è¿”å›çš„ reward æ˜¯å½’ä¸€åŒ–è¿‡çš„ï¼Œä¸èƒ½ç›´æ¥ç”¨ç®—å‡€å€¼
        # æœ€å¥½æ˜¯æˆ‘ä»¬é‡æ–°è·‘ä¸€éé€»è¾‘ï¼Œæˆ–è€…åœ¨ Wrapper é‡Œè®°å½•çœŸå®æ”¶ç›Š
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ Wrapper æ²¡æ”¹ Rewardï¼Œç›´æ¥ç”¨ env.render æˆ– hack
        
        # æ­£ç¡®åšæ³•ï¼šç›´æ¥å¤ç°é€»è¾‘æˆ–è®© Wrapper è¿”å›çœŸå®æ”¶ç›Š
        # è¿™é‡Œæˆ‘ä»¬å‡å®š base_env.current_step å·²ç»æ¨è¿›äº†
        # æ‹¿åˆ°ä¸Šä¸€æ­¥çš„æ”¶ç›Š
        
        # Hack: ä» Info ä¸­æå– Alpha æƒé‡
        alpha_weights = infos[0]['alpha_weights']
        alpha_weights_history.append(alpha_weights)
        
        # ç®€å•è®¡ç®— Benchmark æ”¶ç›Š (1/N Alpha)
        # ... (ç•¥)
        
        step_idx += 1
        if step_idx >= 900: # ç¨å¾®æå‰ç»“æŸé¿å…è¶Šç•Œ
            break
            
    # --- å¯è§†åŒ–åˆ†æ ---
    alpha_weights_history = np.array(alpha_weights_history)
    
    plt.figure(figsize=(12, 8))
    
    # å›¾1: Alpha æƒé‡åˆ†é…çƒ­åŠ›å›¾ (The Money Shot!)
    plt.subplot(2, 1, 1)
    plt.title("Meta-Agent Decision: Which Alpha to Trust?")
    # Alpha 0: Oracle, Alpha 1: Inverse, Alpha 2: Noise, Alpha 3: Regime
    labels = ["Oracle", "Inverse", "Noise", "Regime"]
    
    plt.stackplot(range(len(alpha_weights_history)), alpha_weights_history.T, labels=labels, alpha=0.8)
    plt.legend(loc='upper left')
    plt.ylabel("Weight Allocation")
    plt.xlabel("Time Step")
    
    # å›¾2: å¸‚åœºçŠ¶æ€ (Regime)
    # æˆ‘ä»¬æŠŠçœŸå®çš„ Regime ç”»å‡ºæ¥ï¼Œçœ‹çœ‹ Agent æœ‰æ²¡æœ‰åœ¨ Bear æ—¶åˆ‡æ¢ç­–ç•¥
    regimes = base_env.regimes[:len(alpha_weights_history)]
    plt.subplot(2, 1, 2)
    plt.title("True Market Regime (0: Bull, 1: Bear)")
    plt.plot(regimes, color='black', drawstyle='steps-post', lw=1)
    plt.fill_between(range(len(regimes)), 0, regimes, color='gray', alpha=0.3)
    plt.xlabel("Time Step")
    
    plt.tight_layout()
    plt.savefig("meta_dmpo_analysis.png")
    print("ğŸ“ˆ åˆ†æå›¾å·²ä¿å­˜ä¸º meta_dmpo_analysis.png")
    print("  -> è§‚å¯Ÿå›¾1ï¼šå¦‚æœ 'Oracle' (è“è‰²) å æ®ä¸»å¯¼ï¼Œè¯´æ˜ Agent å­¦ä¼šäº†ã€‚")
    print("  -> è§‚å¯Ÿå›¾2ï¼šå¦‚æœåœ¨ 'Bear' é˜´å½±åŒºï¼ŒAgent å¢åŠ äº† 'Inverse' æˆ– 'Regime' çš„æƒé‡ï¼Œè¯´æ˜å®ƒå­¦ä¼šäº†æ‹©æ—¶ã€‚")

if __name__ == "__main__":
    run_test()