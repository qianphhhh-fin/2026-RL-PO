import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import os

# å¿…é¡»å¯¼å…¥ PPOï¼Œå› ä¸ºæˆ‘ä»¬æ¢å›äº† PPO
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from meta_env import MetaExecutionEnv

# ------------------------------------------------------------------------
# è¾…åŠ©å‡½æ•°ï¼šè¿è¡Œå•æ¬¡ç­–ç•¥å›æµ‹
# ------------------------------------------------------------------------
def run_strategy(env, model=None, static_action=None, seed=42, label="Strategy"):
    """
    è¿è¡Œç­–ç•¥å¹¶è¿”å›å‡€å€¼æ›²çº¿å’Œç›¸å…³æ•°æ®ã€‚
    å…³é”®ï¼šæ¯æ¬¡è¿è¡Œå‰é‡ç½®ç›¸åŒçš„ç§å­ï¼Œä¿è¯æ‰€æœ‰ç­–ç•¥é¢å¯¹çš„æ˜¯å®Œå…¨ç›¸åŒçš„å¸‚åœºè¡Œæƒ…ã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨è¿è¡Œç­–ç•¥: {label} ...")
    
    # 1. é‡ç½®ç¯å¢ƒ (å›ºå®šç§å­)
    # VecEnv çš„ reset ä¸æ¥å— seedï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è°ƒç”¨å†…éƒ¨ env çš„ reset
    # env.envs[0] æ˜¯ DummyVecEnv åŒ…è£¹çš„åŸå§‹ç¯å¢ƒ (MetaExecutionEnv)
    raw_env = env.envs[0]
    
    # è¿™é‡Œæˆ‘ä»¬æ˜¾å¼è°ƒç”¨ raw_env.reset æ¥æ§åˆ¶ç§å­
    # ç„¶åè®© VecEnv åŒæ­¥ä¸€ä¸‹ obs (é€šè¿‡ env.reset() å†æ¬¡è§¦å‘ï¼Œä½†å› ä¸º DataGenerator æ˜¯ rng æ§åˆ¶çš„ï¼Œ
    # æˆ‘ä»¬éœ€è¦åœ¨ raw_env å±‚é¢é‡æ–°ç”Ÿæˆ rng æˆ–è€…ç¡®ä¿ reset é€»è¾‘ä¸€è‡´)
    
    # æœ€ç¨³å¦¥çš„æ–¹æ³•ï¼šç›´æ¥è°ƒç”¨ env.reset()ï¼Œä½†åœ¨ MetaEnv å†…éƒ¨ç¡®ä¿ seed ç”Ÿæ•ˆ
    # æˆ‘ä»¬é€šè¿‡ hack æ–¹å¼ï¼š
    raw_env.reset(seed=seed) 
    obs = env.reset() # è¿™ä¼šå†æ¬¡è°ƒç”¨å†…éƒ¨ resetï¼Œä½†å¦‚æœæ˜¯ DummyVecEnvï¼Œå®ƒåªæ˜¯è·å–è¿”å›å€¼
    
    # ä¸ºäº†åŒé‡ä¿é™©ï¼Œå†æ¬¡å¼ºåˆ¶é‡ç½® DataGenerator çš„çŠ¶æ€
    # (åœ¨ MetaExecutionEnv v2 ä¸­ï¼Œreset(seed) ä¼šé‡ç½® data_gen)
    
    terminated = False
    truncated = False
    
    wealth = [1.0]
    actions = []
    regrets = []
    net_returns = []
    
    while not (terminated or truncated):
        if model:
            # RL é¢„æµ‹ (ç¡®å®šæ€§æ¨¡å¼)
            action, _ = model.predict(obs, deterministic=True)
        else:
            # é™æ€ç­–ç•¥ï¼šéœ€è¦å°† (3,) æ‰©å±•ä¸º (1, 3) é€‚é… VecEnv
            action = np.array([static_action])
            
        obs, rewards, dones, infos = env.step(action)
        
        info = infos[0]
        terminated = dones[0]
        truncated = info.get("TimeLimit.truncated", False)
        
        # è®°å½•æ•°æ®
        r_net = info['net_return']
        wealth.append(wealth[-1] * (1 + r_net))
        net_returns.append(r_net)
        regrets.append(info['regret'])
        
        if model:
            actions.append(action[0])
            
    # æå– Ground Truth (God Mode) æ•°æ®
    # å› ä¸ºæˆ‘ä»¬å›ºå®šäº†ç§å­ï¼Œç¯å¢ƒå†…éƒ¨è®¡ç®—çš„ GT ä¹Ÿæ˜¯é’ˆå¯¹å½“å‰è¡Œæƒ…çš„
    gt_weights = raw_env.ground_truth_w
    real_returns = raw_env.real_returns
    
    return wealth, np.array(actions), np.sum(regrets), gt_weights, real_returns

# ------------------------------------------------------------------------
# ä¸»å‡½æ•°
# ------------------------------------------------------------------------
def main():
    print("ğŸš€ åˆå§‹åŒ–æµ‹è¯•æµç¨‹ (v2.0 PPO + LogSpace)...")
    
    # é…ç½®è·¯å¾„
    MODEL_NAME = "meta_execution_ppo_v3" # å¯¹åº” train.py ä¸­çš„åç§°
    VECNORM_PATH = f"{MODEL_NAME}_vecnorm.pkl"
    
    if not os.path.exists(f"{MODEL_NAME}.zip"):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_NAME}.zipï¼Œè¯·å…ˆè¿è¡Œ train.py")
        return

    # 1. åˆ›å»ºç¯å¢ƒ
    # æµ‹è¯•æ—¶ä½¿ç”¨ DummyVecEnvï¼Œå› ä¸º VecNormalize éœ€è¦å®ƒ
    # æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª lambda å·¥å‚
    env_maker = lambda: MetaExecutionEnv(n_assets=5, episode_length=100, cost_rate=0.0005)
    env = DummyVecEnv([env_maker])
    
    # 2. åŠ è½½å½’ä¸€åŒ–å‚æ•° (VecNormalize)
    # è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼å¦‚æœæ²¡åŠ è½½ï¼Œæ¨¡å‹å°±æ˜¯çå­
    try:
        env = VecNormalize.load(VECNORM_PATH, env)
        env.training = False     # æµ‹è¯•æ¨¡å¼ï¼šä¸æ›´æ–°å‡å€¼æ–¹å·®
        env.norm_reward = False  # æµ‹è¯•æ¨¡å¼ï¼šä¸å½’ä¸€åŒ–å¥–åŠ±ï¼Œæˆ‘ä»¬è¦çœ‹çœŸå®æ”¶ç›Š
        print(f"âœ… æˆåŠŸåŠ è½½ Observation å½’ä¸€åŒ–å‚æ•°: {VECNORM_PATH}")
    except Exception as e:
        print(f"âŒ è­¦å‘Šï¼šæ— æ³•åŠ è½½å½’ä¸€åŒ–å‚æ•° ({e})ã€‚å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œæˆ–æ²¡ä¿å­˜è¿‡pklï¼Œè¯·å¿½ç•¥ã€‚")
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œæœ€å¥½ä¸è¦ç»§ç»­ï¼Œå› ä¸ºè§‚æµ‹ç©ºé—´åˆ†å¸ƒå®Œå…¨ä¸åŒ
        return

    # 3. åŠ è½½ PPO æ¨¡å‹
    model = PPO.load(MODEL_NAME)
    print(f"âœ… æˆåŠŸåŠ è½½ PPO æ¨¡å‹: {MODEL_NAME}")
    
    TEST_SEED = 2026 # å›ºå®šæµ‹è¯•ç§å­
    
    # -------------------------------------------------------
    # A. è¿è¡Œ RL Agent (Ours)
    # -------------------------------------------------------
    wealth_rl, actions_rl, regret_rl, _, _ = run_strategy(
        env, model=model, seed=TEST_SEED, label="Meta-RL Agent"
    )
    
    # -------------------------------------------------------
    # B. è¿è¡Œ Static Benchmark (Baseline)
    # -------------------------------------------------------
    # æˆ‘ä»¬éœ€è¦æŠŠç‰©ç†å‚æ•°è½¬æ¢ä¸º Log-Space åŠ¨ä½œ
    # ç›®æ ‡ç‰©ç†å‚æ•°: Lambda=5.0, Gamma=0.005, Kappa=1.0
    # æ˜ å°„å…¬å¼å›é¡¾: 
    #   Lambda = exp(a[0] * 4.6)      => a[0] = ln(Lambda) / 4.6
    #   Gamma  = exp(a[1]*4.6 - 4.6)  => a[1] = (ln(Gamma) + 4.6) / 4.6
    #   Kappa  = a[2] + 1.0           => a[2] = Kappa - 1.0
    
    target_lambda = 5.0
    target_gamma = 0.005
    target_kappa = 1.0
    
    static_a0 = np.log(target_lambda) / 4.6
    static_a1 = (np.log(target_gamma) + 4.6) / 4.6
    static_a2 = target_kappa - 1.0
    
    static_action = np.array([static_a0, static_a1, static_a2])
    print(f"â„¹ï¸  Static Benchmark Action (Log Space): {static_action}")
    
    wealth_static, _, regret_static, gt_weights, r_real = run_strategy(
        env, static_action=static_action, seed=TEST_SEED, label="Static Benchmark"
    )
    
    # -------------------------------------------------------
    # C. è®¡ç®— Ground Truth (God Mode) å‡€å€¼æ›²çº¿
    # -------------------------------------------------------
    # æˆ‘ä»¬é‡æ–°è®¡ç®—ä¸€é GT çš„å¤åˆ©å‡€å€¼ï¼Œä»¥ç¡®ä¿æ—¶é—´è½´å¯¹é½
    gt_wealth = [1.0]
    w_prev = np.ones(5) / 5
    cost_rate = 0.0005
    
    for t in range(len(r_real)):
        # 1. è®¡ç®—æ¢æ‰‹æˆæœ¬
        turnover = np.sum(np.abs(gt_weights[t] - w_prev))
        cost = turnover * cost_rate
        # 2. è®¡ç®—å‡€æ”¶ç›Š
        ret = np.dot(gt_weights[t], r_real[t]) - cost
        # 3. å¤åˆ©
        gt_wealth.append(gt_wealth[-1] * (1 + ret))
        w_prev = gt_weights[t]
        
    # -------------------------------------------------------
    # 4. ç»˜å›¾åˆ†æ
    # -------------------------------------------------------
    print(f"\nğŸ“Š æœ€ç»ˆå›æµ‹ç»Ÿè®¡ (Episode Length: 100):")
    print(f"God Mode Final Wealth:     {gt_wealth[-1]:.4f}")
    print(f"RL Agent Final Wealth:     {wealth_rl[-1]:.4f}")
    print(f"Static Bench Final Wealth: {wealth_static[-1]:.4f}")
    print(f"Efficiency (RL / God):     {wealth_rl[-1] / gt_wealth[-1] * 100:.2f}%")
    
    plt.figure(figsize=(16, 10))
    
    # --- å›¾ 1: å‡€å€¼æ›²çº¿ ---
    plt.subplot(2, 2, 1)
    plt.plot(gt_wealth, 'r--', label='Risk-Adjusted God (Ceiling)', alpha=0.6)
    plt.plot(wealth_rl, 'g-', label='Meta-RL Agent (Ours)', linewidth=2.5)
    plt.plot(wealth_static, 'b-', label='Static Benchmark', alpha=0.6)
    plt.title('Wealth Curve Comparison')
    plt.ylabel('Net Worth')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # --- å›¾ 2: RL å‚æ•°åŠ¨æ€è°ƒæ•´ (è¿˜åŸä¸ºç‰©ç†æ„ä¹‰) ---
    plt.subplot(2, 2, 2)
    if len(actions_rl) > 0:
        # è¿˜åŸæ˜ å°„
        # Lambda = exp(a * 4.6)
        risk_aversion = np.exp(actions_rl[:, 0] * 4.6)
        # Gamma = exp(a * 4.6 - 4.6)
        trade_penalty = np.exp(actions_rl[:, 1] * 4.6 - 4.6)
        # Kappa = a + 1
        alpha_conf = actions_rl[:, 2] + 1.0
        
        # åŒè½´ç»˜å›¾
        ax1 = plt.gca()
        line1 = ax1.plot(risk_aversion, color='purple', label='Risk Aversion ($\lambda$)', alpha=0.8)
        ax1.set_ylabel('Risk Aversion ($\lambda$)', color='purple')
        ax1.set_yscale('log') # Lambda å˜åŒ–èŒƒå›´å¤§ï¼Œç”¨å¯¹æ•°è½´æ›´å¥½çœ‹
        
        ax2 = ax1.twinx()
        # æ¢æ‰‹æƒ©ç½šä¹˜ 10000 å˜æˆ bps å•ä½
        line2 = ax2.plot(trade_penalty * 10000, color='orange', label='Trade Penalty (bps)', alpha=0.8)
        ax2.set_ylabel('Trade Penalty (bps)', color='orange')
        
        # å›¾ä¾‹åˆå¹¶
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax1.legend(lines, labels, loc='upper left')
        plt.title('RL Dynamic Parameter Tuning (Physical Scale)')
    
    # --- å›¾ 3: ç›¸å¯¹é—æ†¾å€¼ Gap ---
    plt.subplot(2, 2, 3)
    # è®¡ç®—ç›¸å¯¹ God çš„ç™¾åˆ†æ¯”å·®è·
    rl_gap = (np.array(gt_wealth) - np.array(wealth_rl)) / np.array(gt_wealth) * 100
    static_gap = (np.array(gt_wealth) - np.array(wealth_static)) / np.array(gt_wealth) * 100
    
    plt.plot(rl_gap, 'g', label='RL Drawdown vs God (%)')
    plt.plot(static_gap, 'b', label='Static Drawdown vs God (%)')
    plt.title('Performance Gap to God Mode (Lower is Better)')
    plt.ylabel('Gap (%)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
# --- å›¾ 4: å¸‚åœºç¯å¢ƒ (æ³¢åŠ¨ç‡) ---
    plt.subplot(2, 2, 4)
    
    # ä¿®å¤ï¼šä» VecEnv ä¸­æå–åŸå§‹ç¯å¢ƒå®ä¾‹
    raw_env = env.envs[0]
    
    # ä» raw_env è·å–è¿™ä¸€è½®çš„æ³¢åŠ¨ç‡æ•°æ®
    vols = np.mean(np.sqrt(np.diagonal(raw_env.sigmas, axis1=1, axis2=2)), axis=1)
    # æˆªå–å‰ 100 æ­¥
    vols = vols[:len(actions_rl)]
    
    plt.plot(vols, 'k-', alpha=0.6, label='Avg Market Volatility')
    plt.fill_between(range(len(vols)), vols, 0, color='gray', alpha=0.1)
    plt.title('Market Context: Volatility Regime')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()