import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from sbx import SAC
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from meta_env import MetaExecutionEnv

def run_strategy(env, model=None, static_action=None, label="Strategy"):
    """
    è¿è¡Œç­–ç•¥å›æµ‹
    æ³¨æ„ï¼šè¿™é‡Œçš„ env å·²ç»æ˜¯è¢« VecNormalize åŒ…è£¹è¿‡çš„
    """
    # å¼ºåˆ¶å›ºå®šç§å­ï¼Œç¡®ä¿å¯¹æ¯”å…¬å¹³
    # VecEnv çš„ reset ä¸éœ€è¦ seed å‚æ•°ï¼Œå®ƒåœ¨å†…éƒ¨ç®¡ç†
    obs = env.reset()
    
    # è¿™é‡Œçš„ env æ˜¯ VecEnvï¼Œæ‰€ä»¥ step è¿”å›çš„æ˜¯æ•°ç»„ï¼Œæˆ‘ä»¬éœ€è¦å–ç¬¬ä¸€ä¸ªå…ƒç´ 
    terminated = False
    truncated = False
    
    wealth = [1.0]
    actions = []
    regrets = []
    
    # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è®¿é—®å†…éƒ¨çš„åŸå§‹ç¯å¢ƒæ¥è·å– info ä¸­çš„ net_return (æœªå½’ä¸€åŒ–çš„çœŸå®å€¼)
    # å› ä¸º VecNormalize å¯èƒ½ä¼šä¿®æ”¹ rewardï¼Œè™½ç„¶æˆ‘ä»¬è®¾äº† norm_reward=Falseï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œ
    # æˆ‘ä»¬ç›´æ¥ä» info é‡Œè¯»åŸå§‹æ•°æ®
    
    while not (terminated or truncated):
        if model:
            # deterministic=True æ„å‘³ç€æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆä¸åŠ å™ªå£°ï¼‰
            action, _ = model.predict(obs, deterministic=True)
        else:
            # é™æ€ç­–ç•¥éœ€è¦æ‰©å±•ç»´åº¦ä»¥é€‚é… VecEnv: (3,) -> (1, 3)
            action = np.array([static_action])
            
        obs, rewards, dones, infos = env.step(action)
        
        info = infos[0] # å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„ info
        terminated = dones[0]
        truncated = info.get("TimeLimit.truncated", False)
        
        # è®°å½•çœŸå®å‡€å€¼å˜åŒ– (ä½¿ç”¨ info ä¸­çš„çœŸå®å›æŠ¥ï¼Œä¸å— reward scaling å½±å“)
        wealth.append(wealth[-1] * (1 + info['net_return']))
        
        if model:
            actions.append(action[0]) # è®°å½• RL çš„åŠ¨ä½œ
        regrets.append(info['regret'])
        
    # è·å– Ground Truth æƒé‡ (ä»å†…éƒ¨ç¯å¢ƒæå–)
    # env -> VecNormalize -> DummyVecEnv -> MetaExecutionEnv
    raw_env = env.envs[0]
    gt_weights = raw_env.ground_truth_w
    real_returns = raw_env.real_returns
    
    return wealth, np.array(actions), np.sum(regrets), gt_weights, real_returns, raw_env

def main():
    print("ğŸš€ åŠ è½½æµ‹è¯•ç¯å¢ƒ...")
    
    # 1. åˆ›å»ºåŸºç¡€ç¯å¢ƒ (æµ‹è¯•æ—¶ä¸éœ€è¦ RewardScaleï¼Œæˆ‘ä»¬è¦çœ‹çœŸå®çš„ä¸€åˆ†ä¸€æ¯«)
    # ä½†å¿…é¡»ä½¿ç”¨ DummyVecEnvï¼Œå› ä¸º VecNormalize éœ€è¦å®ƒ
    base_env = MetaExecutionEnv(n_assets=5, episode_length=100, cost_rate=0.0005)
    env = DummyVecEnv([lambda: base_env])
    
    # 2. åŠ è½½å½’ä¸€åŒ–å‚æ•° (å…³é”®æ­¥éª¤ï¼)
    model_name = "meta_execution_sac_sbx"
    vecnorm_path = f"{model_name}_vecnorm.pkl"
    
    try:
        # åŠ è½½ç»Ÿè®¡æ•°æ® (å‡å€¼/æ–¹å·®)
        env = VecNormalize.load(vecnorm_path, env)
        # æµ‹è¯•æ¨¡å¼ï¼šä¸è¦æ›´æ–°å‡å€¼å’Œæ–¹å·®
        env.training = False 
        # æµ‹è¯•æ¨¡å¼ï¼šä¸è¦å½’ä¸€åŒ– Reward (è™½ç„¶è®­ç»ƒæ—¶ä¹Ÿæ²¡å½’ä¸€åŒ–ï¼Œä½†è¿™é‡Œæ˜¾å¼å…³é—­æ›´å®‰å…¨)
        env.norm_reward = False
        print(f"âœ… æˆåŠŸåŠ è½½ Observation å½’ä¸€åŒ–å‚æ•°: {vecnorm_path}")
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½å½’ä¸€åŒ–å‚æ•° ({e})ã€‚è¯·ç¡®ä¿ train.py è¿è¡ŒæˆåŠŸã€‚")
        return

    # 3. åŠ è½½æ¨¡å‹
    try:
        model = SAC.load(model_name)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
    except:
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹ {model_name}")
        return

    print("\nâš”ï¸ å¼€å§‹å¯¹å†³ï¼šRL Agent vs Static Benchmark vs God Mode")

    # --- A. è¿è¡Œ RL Agent ---
    wealth_rl, actions_rl, regret_rl, _, _, _ = run_strategy(
        env, model=model, label="Meta-RL Agent"
    )

    # --- B. è¿è¡Œ Static Benchmark ---
    # Lambda=5.0 -> action[0] approx -0.5
    # Gamma=0.005 -> action[1] approx -0.9
    static_action = np.array([-0.5, -0.9, 0.0])
    wealth_static, _, regret_static, gt_weights, r_real, raw_env = run_strategy(
        env, static_action=static_action, label="Static Benchmark"
    )
    
    # --- C. è®¡ç®— Ground Truth æ›²çº¿ ---
    # æˆ‘ä»¬é‡æ–°è®¡ç®—ä¸€é GT å‡€å€¼ï¼Œç¡®ä¿å¯¹é½
    gt_wealth = [1.0]
    w_prev = np.ones(5) / 5
    for t in range(len(r_real)):
        turnover = np.sum(np.abs(gt_weights[t] - w_prev))
        cost = turnover * 0.0005
        ret = np.dot(gt_weights[t], r_real[t]) - cost
        gt_wealth.append(gt_wealth[-1] * (1 + ret))
        w_prev = gt_weights[t]

    # --- 4. ç»˜å›¾åˆ†æ ---
    print(f"\nğŸ“Š æœ€ç»ˆå›æµ‹ç»“æœ (Episode Length: 100):")
    print(f"Ground Truth Final Wealth: {gt_wealth[-1]:.4f}")
    print(f"RL Agent Final Wealth:     {wealth_rl[-1]:.4f}")
    print(f"Static Bench Final Wealth: {wealth_static[-1]:.4f}")
    
    plt.figure(figsize=(16, 10))
    
    # å›¾1: å‡€å€¼èµ°åŠ¿
    plt.subplot(2, 2, 1)
    plt.plot(gt_wealth, 'r--', label='Risk-Adjusted God (Ceiling)', alpha=0.6)
    plt.plot(wealth_rl, 'g-', label='Meta-RL Agent (Ours)', linewidth=2)
    plt.plot(wealth_static, 'b-', label='Static Benchmark', alpha=0.6)
    plt.title('Wealth Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å›¾2: RL å‚æ•°åŠ¨æ€è°ƒæ•´
    plt.subplot(2, 2, 2)
    if len(actions_rl) > 0:
        # è§£æåŠ¨ä½œåˆ°ç‰©ç†å‚æ•°
        risk_aversion = 0.1 + 19.9 * (0.5 * (actions_rl[:, 0] + 1))
        trade_penalty = 0.10 * (0.5 * (actions_rl[:, 1] + 1))
        
        plt.plot(risk_aversion, color='purple', label='Risk Aversion ($\lambda$)')
        plt.ylabel('Risk Aversion', color='purple')
        plt.legend(loc='upper left')
        
        ax2 = plt.gca().twinx()
        ax2.plot(trade_penalty * 10000, color='orange', label='Trade Penalty (bps)', alpha=0.7)
        ax2.set_ylabel('Trade Penalty (bps)', color='orange')
        ax2.legend(loc='upper right')
        plt.title('RL Dynamic Parameter Tuning')
    
    # å›¾3: ç´¯ç§¯é—æ†¾å€¼ (è¶Šä½è¶Šå¥½)
    plt.subplot(2, 2, 3)
    rl_gap = np.array(gt_wealth) - np.array(wealth_rl)
    static_gap = np.array(gt_wealth) - np.array(wealth_static)
    plt.plot(rl_gap, 'g', label='RL Wealth Gap (to God)')
    plt.plot(static_gap, 'b', label='Static Wealth Gap (to God)')
    plt.title('Wealth Gap vs God Mode (Lower is Better)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å›¾4: å¸‚åœºç¯å¢ƒ (æ³¢åŠ¨ç‡)
    plt.subplot(2, 2, 4)
    vols = np.mean(np.sqrt(np.diagonal(raw_env.sigmas, axis1=1, axis2=2)), axis=1)
    # åªå–å‰100æ­¥
    vols = vols[:100]
    plt.plot(vols, 'k-', alpha=0.5, label='Avg Market Volatility')
    plt.title('Market Context (Volatility)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()