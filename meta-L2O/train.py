import gymnasium as gym
import numpy as np
import os
import torch
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, VecNormalize
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList
from stable_baselines3.common.utils import set_random_seed
from sbx import SAC
from meta_env import MetaExecutionEnv

# --- 1. å¥–åŠ±ç¼©æ”¾ Wrapper ---
class RewardScaleWrapper(gym.RewardWrapper):
    def __init__(self, env, scale=100.0):
        super().__init__(env)
        self.scale = scale
        
    def reward(self, reward):
        return reward * self.scale

# --- 2. æ•ˆç‡ç›‘æ§ Callback ---
class EfficiencyCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        # é™ä½é¢‘ç‡åˆ° 1000 step è®°å½•ä¸€æ¬¡ï¼Œå‡å°‘å¼€é”€
        if self.n_calls % 1000 == 0:
            infos = self.locals['infos']
            effs = []
            regrets = []
            lambdas = []
            gammas = []
            
            for info in infos:
                if 'gt_net_return' in info and 'net_return' in info:
                    # é¿å…é™¤ä»¥æå°å€¼
                    if abs(info['gt_net_return']) > 1e-5:
                        effs.append(info['net_return'] / info['gt_net_return'])
                    
                    regrets.append(info['regret'])
                    lambdas.append(info['risk_aversion'])
                    gammas.append(info['trade_penalty'])

            if effs:
                self.logger.record("custom/efficiency_vs_god", np.mean(effs))
            if regrets:
                self.logger.record("custom/regret_mean", np.mean(regrets))
            if lambdas:
                self.logger.record("params/risk_aversion", np.mean(lambdas))
            if gammas:
                self.logger.record("params/trade_penalty", np.mean(gammas))
        return True

def make_env(rank, seed=0):
    def _init():
        env = MetaExecutionEnv(n_assets=5, episode_length=50, cost_rate=0.0005)
        # æ ¸å¿ƒä¿®æ”¹ï¼šæ”¾å¤§å¥–åŠ±
        env = RewardScaleWrapper(env, scale=100.0) 
        env.reset(seed=seed + rank)
        return env
    return _init

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    N_ENVS = 8
    TOTAL_TIMESTEPS = 300_000 
    
    # ä¿æŒåŸæ¥çš„ log ç›®å½•ä¸å˜
    LOG_DIR = "./logs/meta_sac_sbx/"
    # ä¿æŒåŸæ¥çš„æ¨¡å‹åå­—ä¸å˜
    MODEL_PATH = "meta_execution_sac_sbx"
    
    os.makedirs(LOG_DIR, exist_ok=True)
    set_random_seed(42)

    print(f"ğŸš€ å¯åŠ¨ {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒ (Reward Scaled x100)...")

    # --- 1. åˆ›å»ºç¯å¢ƒ ---
    env = SubprocVecEnv([make_env(i) for i in range(N_ENVS)])
    
    # --- 2. æ ¸å¿ƒä¿®æ”¹ï¼šè‡ªåŠ¨å½’ä¸€åŒ– Observation ---
    # norm_reward=False æ˜¯å› ä¸ºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨ Scale äº†
    # clip_obs=10.0 é˜²æ­¢å¼‚å¸¸å€¼å¹²æ‰°ç½‘ç»œ
    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)
    
    # ç›‘æ§å™¨
    env = VecMonitor(env, LOG_DIR)

    # --- 3. åˆå§‹åŒ–æ¨¡å‹ ---
    model = SAC(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=3e-4,
        buffer_size=100_000,
        batch_size=256,
        # æ ¸å¿ƒä¿®æ”¹ï¼šå›ºå®š Entropy ä¸º 0.05ï¼Œå¼ºåˆ¶å®ƒæ¢ç´¢ï¼Œä¸å‡†èººå¹³
        ent_coef='auto', 
        gamma=0.99,
        tau=0.005,
        tensorboard_log=LOG_DIR,
        policy_kwargs=dict(net_arch=[256, 256]),
    )

    print("ğŸƒ å¼€å§‹è®­ç»ƒ (Fixed Entropy + Normalized Obs)...")
    
    # ç»„åˆ Callbacks (æ•ˆç‡ç›‘æ§ + è‡ªåŠ¨ä¿å­˜Checkpointé˜²æ­¢æ„å¤–)
    eff_callback = EfficiencyCallback()
    checkpoint_callback = CheckpointCallback(save_freq=50000, save_path=LOG_DIR, name_prefix='ckpt')
    
    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=CallbackList([eff_callback, checkpoint_callback]), progress_bar=True)

    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œä¿å­˜æ¨¡å‹è‡³ {MODEL_PATH}...")
    model.save(MODEL_PATH)
    
    # é‡è¦ï¼šå¿…é¡»ä¿å­˜ VecNormalize çš„ç»Ÿè®¡æ•°æ® (å‡å€¼å’Œæ–¹å·®)ï¼Œå¦åˆ™æµ‹è¯•æ—¶æ¨¡å‹å°±æ˜¯çå­
    env.save(f"{MODEL_PATH}_vecnorm.pkl")
    
    env.close()