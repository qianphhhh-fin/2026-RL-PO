import numpy as np
import matplotlib.pyplot as plt
import cvxpy as cp
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

from env import PortfolioEnv
from dmpo_model import DMPOActionWrapper

# --- Benchmark: å¸¦çº¦æŸçš„å‡å€¼æ–¹å·® ---
def run_constrained_mv(returns, lookback=30, max_turnover=0.10):
    T, N = returns.shape
    weights = np.ones(N) / N
    equity = [1.0]
    
    for t in range(lookback, T):
        # ä¼°è®¡å‚æ•°
        window = returns[t-lookback:t]
        mu = np.mean(window, axis=0)
        Sigma = np.cov(window.T)
        
        # æ±‚è§£å¸¦çº¦æŸ QP
        w = cp.Variable(N)
        w_prev = cp.Parameter(N); w_prev.value = weights
        
        # ç›®æ ‡: Max Return - Risk
        obj = cp.Maximize(mu @ w - 0.5 * cp.quad_form(w, Sigma))
        cons = [
            cp.sum(w) == 1, 
            w >= 0,
            cp.norm(w - w_prev, 1) <= max_turnover # å…¬å¹³å¯¹æ¯”!
        ]
        
        prob = cp.Problem(obj, cons)
        try:
            prob.solve(solver=cp.OSQP, eps_abs=1e-4, eps_rel=1e-4)
            if w.value is not None:
                weights = np.maximum(w.value, 0)
                weights /= np.sum(weights)
        except:
            pass
        
        # è®¡ç®—å‡€å€¼
        r = np.sum(weights * returns[t]) - max_turnover * 0.0005 # ä¼°ç®—æˆæœ¬
        equity.append(equity[-1] * (1 + r))
        
    return np.array(equity)

def main():
    # 1. åˆ›å»ºæµ‹è¯•ç¯å¢ƒ (ä¿æŒéšæœºç§å­å›ºå®šä»¥å¤ç°)
    # æ³¨æ„: è¿™é‡Œä¸éœ€è¦é‡æ–°ç”Ÿæˆæ•°æ®ï¼Œæˆ‘ä»¬resetåè·å–æ•°æ®
    raw_env = PortfolioEnv(n_assets=10, lookback=30, max_turnover=0.10)
    
    # 2. åŒ…è£…ç¯å¢ƒ (ç”¨äºæ¨¡å‹é¢„æµ‹)
    def make_test_env():
        e = PortfolioEnv(n_assets=10, lookback=30, max_turnover=0.10)
        e = DMPOActionWrapper(e, max_turnover=0.10)
        # å¼ºåˆ¶åŒæ­¥æ•°æ®ï¼Œç¡®ä¿å’Œ raw_env ä½¿ç”¨åŒä¸€å¥—"å¹³è¡Œå®‡å®™"
        e.env.generator = raw_env.generator
        e.env.prices = raw_env.prices
        e.env.returns = raw_env.returns
        e.env.factors = raw_env.factors
        e.env.regimes = raw_env.regimes
        return e

    env = DummyVecEnv([make_test_env])
    
    # 3. åŠ è½½å½’ä¸€åŒ–å‚æ•° (Training=False)
    try:
        env = VecNormalize.load("vec_normalize.pkl", env)
        env.training = False
        env.norm_reward = False
    except:
        print("âš ï¸ æœªæ‰¾åˆ°å½’ä¸€åŒ–å‚æ•°ï¼Œç»“æœå¯èƒ½ä¸å‡†")

    # 4. åŠ è½½æ¨¡å‹
    model = PPO.load("dmpo_transformer_agent", env=env)
    
    # --- è¿è¡Œå›æµ‹ ---
    print("ğŸ“Š è¿è¡Œ DMPO å›æµ‹...")
    obs = env.reset()
    dmpo_equity = [1.0]
    dmpo_violations = 0
    
    # è·å–çœŸå®æ•°æ®ç”¨äº Benchmark
    returns_data = raw_env.returns
    
    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, _, dones, infos = env.step(action)
        
        info = infos[0]
        dmpo_equity.append(dmpo_equity[-1] * (1 + info['return']))
        
        if info['turnover'] > 0.10 + 1e-4:
            dmpo_violations += 1
            
        if dones[0]: break
            
    # --- è¿è¡Œ Benchmark ---
    print("ğŸ“Š è¿è¡Œ Constrained Mean-Variance å›æµ‹...")
    mv_equity = run_constrained_mv(returns_data)
    
    # å¯¹é½é•¿åº¦
    min_len = min(len(dmpo_equity), len(mv_equity))
    dmpo_equity = dmpo_equity[:min_len]
    mv_equity = mv_equity[:min_len]
    
    # --- ç»“æœ ---
    print("\n" + "="*40)
    print(f"DMPO Return: {(dmpo_equity[-1]-1):.2%}")
    print(f"MV Return:   {(mv_equity[-1]-1):.2%}")
    print(f"Constraint Violations: {dmpo_violations}")
    print("="*40)
    
    plt.plot(dmpo_equity, label='DMPO (Transformer+QP)')
    plt.plot(mv_equity, label='Mean-Variance (Constrained)', linestyle='--')
    plt.legend()
    plt.title("Backtest: DMPO vs Constrained MV")
    plt.grid(True)
    plt.savefig("final_result.png")
    print("âœ… ç»“æœå›¾å·²ä¿å­˜")

if __name__ == "__main__":
    main()