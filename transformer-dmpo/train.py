import os
import torch as th
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor

from env import PortfolioEnv
from dmpo_model import PortfolioTransformerExtractor, DMPOActionWrapper

def main():
    log_dir = "./dmpo_logs/"
    os.makedirs(log_dir, exist_ok=True)
    
    # 1. ç»„è£…ç¯å¢ƒ
    def make_env():
        env = PortfolioEnv(n_assets=10, lookback=30, max_turnover=0.10)
        env = DMPOActionWrapper(env, max_turnover=0.10)
        return Monitor(env, log_dir)
    
    env = DummyVecEnv([make_env])
    
    # 2. å½’ä¸€åŒ– (å…³é”®!)
    # é‡‘èæ•°æ®é€šå¸¸å¾ˆå° (1e-3)ï¼Œå¿…é¡»å½’ä¸€åŒ– Obs å’Œ Reward
    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10., clip_reward=10.)
    
    # 3. åˆå§‹åŒ– PPO
    model = PPO(
        "MultiInputPolicy",
        env,
        policy_kwargs={
            "features_extractor_class": PortfolioTransformerExtractor,
            "features_extractor_kwargs": {"features_dim": 64},
            "net_arch": dict(pi=[64, 64], vf=[64, 64]),
            "activation_fn": th.nn.Tanh 
        },
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        ent_coef=0.01, # é¼“åŠ±æ¢ç´¢
        verbose=1,
        tensorboard_log=log_dir
    )
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ Transformer-DMPO (Simulated FF5 Data)...")
    try:
        # å»ºè®®è®­ç»ƒè‡³å°‘ 100k æ­¥
        model.learn(total_timesteps=100000)
        
        model.save("dmpo_transformer_agent")
        env.save("vec_normalize.pkl")
        print("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    main()